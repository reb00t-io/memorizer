# CLAUDE.md

## Project Overview

**Memorizer** is a Python library that provides a structured, long‑term memory architecture for large language models with a **model‑managed context**, message timestamps, and background compression to keep long conversations compact.

The core idea is a **fixed‑order context layout** that separates:

1. **System message** – static prompt defining the assistant’s role.
2. **Long‑term memory** – compressed, persistent memory that is updated nightly via a LoRA‑augmented compression loop.
3. **Short‑term memory** – recent interactions kept in memory for the current session.
4. **Recall memory** – optional, on‑demand retrieval of older information.
5. **Working memory** – the active conversation that drives generation.

The `Context` class (see `src/model/context.py`) orchestrates these sections, while the `Model` class (see `src/model/model.py`) handles streaming requests, synchronous calls, and compression utilities.

## Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/memorizer.git
cd memorizer

# Create a virtual environment (optional but recommended)
python3 -m venv .venv
source .venv/bin/activate

# Install the package and its dependencies
pip install .
```

The project requires **Python 3.12+** and the following runtime dependencies (as declared in `pyproject.toml`):

- `requests>=2.31`
- `openai>=1.0`
- `prompt-toolkit>=3.0`

Development dependencies include `pytest` for running the test suite.

## Quick Start

The package ships with a simple interactive chat interface (`src/chat/chat.py`). To launch it:

```bash
python -m src.chat.chat
```

This starts a REPL where you can type messages. The system automatically:

- Stores user and assistant messages in the **working** memory section.
- Calls the `stream_completion` function to obtain a response from the configured `Model`.
- Compresses older assistant messages in the background and stores them as `compressed_content`.

## Core Components

| Module | Purpose |
|--------|---------|
| `src/model/context.py` | Defines the `Context` data structure with fixed memory sections and timestamped rendering. |
| `src/model/memory.py` | Message storage, persistence, and configurable uncompressed tail handling. |
| `src/model/model.py` | Model configuration, streaming, synchronous calls, and compression helpers. |
| `src/chat/completion.py` | Streaming completion loop used by the chat interface. |
| `src/chat/chat.py` | Interactive command‑line interface built with `prompt_toolkit`. |

## Persisted Data

By default, all memory sections are stored under `~/.memorizer/` as JSON files:

- `system_memory.json`
- `long_term_memory.json`
- `short_term_memory.json`
- `recall_memory.json`
- `working_memory.json`

You can change the location by passing a custom `data_dir` to `Context.create()`.

## Testing

Run the test suite with:

```bash
pytest src
```

The tests cover context creation, message handling, and compression behaviour.

## License

This project is released under the **MIT License**.

---

*Generated by Claude Code*
