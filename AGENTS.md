# CLAUDE.md

## Project Overview

**Memorizer** is a Python library that provides a structured, long‑term memory architecture for large language models. It combines **in‑context learning (ICL)** with periodic **LoRA fine‑tuning** so that the model can retain facts, preferences, and behaviors over time without altering the base model weights.

The core idea is a **fixed‑order context layout** that separates:

1. **System message** – static prompt defining the assistant’s role.
2. **Long‑term memory** – compressed, persistent memory that is updated nightly via a LoRA‑augmented compression loop.
3. **Short‑term memory** – recent interactions kept in memory for the current session.
4. **Recall memory** – optional, on‑demand retrieval of older information.
5. **Working memory** – the active conversation that drives generation.

The `Context` class (see `src/context.py`) orchestrates these sections, providing utilities to append messages, compress short‑term + working memory into long‑term memory, and render the full context for a completion request.

## Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/memorizer.git
cd memorizer

# Create a virtual environment (optional but recommended)
python3 -m venv .venv
source .venv/bin/activate

# Install the package and its dependencies
pip install .
```

The project requires **Python 3.12+** and the following runtime dependencies (as declared in `pyproject.toml`):

- `requests>=2.31`
- `openai>=1.0`
- `prompt-toolkit>=3.0`

Development dependencies include `pytest` for running the test suite.

## Quick Start

The package ships with a simple interactive chat interface (`src/chat.py`). To launch it:

```bash
python -m src.chat
```

This starts a REPL where you can type messages. The system automatically:

- Stores user and assistant messages in the **working** memory section.
- Calls the `stream_completion` function to obtain a response from the configured LLM.
- Allows you to compress the short‑term conversation into **long‑term** memory using `Context.compress()` (called manually or via a scheduled job).

## Core Components

| Module | Purpose |
|--------|---------|
| `src/context.py` | Defines the `Context` data structure with fixed memory sections and compression logic. |
| `src/memory.py` | Lightweight wrapper handling role‑based message storage and persistence to JSON files. |
| `src/completion.py` | Provides streaming LLM completion utilities used by the chat loop. |
| `src/llm_nostream.py` | Synchronous LLM call used for summarisation during compression. |
| `src/chat.py` | Interactive command‑line interface built with `prompt_toolkit`. |

## Persisted Data

By default, all memory sections are stored under `~/.memorizer/` as JSON files:

- `system_memory.json`
- `long_term_memory.json`
- `short_term_memory.json`
- `recall_memory.json`
- `working_memory.json`

You can change the location by passing a custom `data_dir` to `Context.create()`.

## Testing

Run the test suite with:

```bash
pytest src
```

The tests cover context creation, message handling, and compression behaviour.

## License

This project is released under the **MIT License**.

---

*Generated by Claude Code*
